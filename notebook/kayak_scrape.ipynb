{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9c2c1df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kayak.com/flights/JFK-ATL/2022-11-30?sort=bestflight_a\n"
     ]
    }
   ],
   "source": [
    "origin = \"JFK\"\n",
    "destination = \"ATL\"\n",
    "startdate = \"2022-11-30\"\n",
    "\n",
    "url1 = \"https://www.kayak.com/flights/\" + origin + \"-\" + destination + \"/\" + startdate + \"?sort=bestflight_a\"\n",
    "url2 = \"https://www.kayak.com/flights/\" + origin + \"-\" + destination + \"/\" + startdate + \"?sort=duration_b\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9,pl;q=0.8\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\"\n",
    "}\n",
    "print(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9fda5b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def scrapeDataFromKayak(url1):\n",
    "    deptime = []\n",
    "    arrtime = []\n",
    "    meridiem = []\n",
    "    airline = []\n",
    "    flight_number = []\n",
    "    price = []\n",
    "    airline = []\n",
    "    \n",
    "    r = requests.get(url1, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    if soup.find_all('p')[0].getText() == \"Please confirm that you are a real KAYAK user.\":\n",
    "        print(\"Kayak thinks I'm a bot, which I am ... so let's wait a bit and try again\")\n",
    "        time.sleep(20)\n",
    "        return \"failure\"\n",
    "\n",
    "    deptimes = soup.find_all('span', attrs={'class': 'depart-time base-time'})\n",
    "    arrtimes = soup.find_all('span', attrs={'class': 'arrival-time base-time'})\n",
    "    meridies = soup.find_all('span', attrs={'class': 'time-meridiem meridiem'})\n",
    "    airlines = soup.find_all('div', attrs={'class': 'bottom'})\n",
    "    flight_number = soup.find_all('div', attrs={'class': 'nAz5-carrier-text'})\n",
    "    price_list = soup.find_all('span', attrs={'class': 'price-text'})\n",
    "    airlines = soup.find_all('div', attrs={'class': 'nAz5-carrier-text'})\n",
    "\n",
    "    for div in deptimes:\n",
    "        deptime.append(div.getText()[:-1])\n",
    "\n",
    "    for div in arrtimes:\n",
    "        arrtime.append(div.getText()[:-1])\n",
    "\n",
    "    for div in meridies:\n",
    "        meridiem.append(div.getText())\n",
    "        \n",
    "    for div in airline:\n",
    "        airline.append(int(div.getText().split('\\n')[1][1:-1].replace(',', '')))\n",
    "\n",
    "    for div in price_list:\n",
    "        price.append(int(div.getText().split('\\n')[1][1:-1].replace(',', '')))\n",
    "        \n",
    "    for div in airlines:\n",
    "        airline.append(div.getText())\n",
    "                \n",
    "#     deptime = np.asarray(deptime)\n",
    "#     arrtime = np.asarray(arrtime)\n",
    "    meridiem = np.asarray(meridiem)\n",
    "    meridiem = meridiem.reshape(int(len(meridiem)/2), 2)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"origin\" : origin,\n",
    "        \"destination\" : destination,\n",
    "        \"startdate\" : startdate,\n",
    "        \"price\": price,\n",
    "        \"currency\": \"USD\",\n",
    "        \"deptime\": [m+str(n[0]) for m,n in zip(deptime,meridiem)],\n",
    "        \"arrtime\": [m+str(n[0]) for m,n in zip(arrtime,meridiem)]\n",
    "#         \"airline\": airline\n",
    "    })\n",
    "#     print(df)\n",
    "    print(airline)\n",
    "    \n",
    "scrapeDataFromKayak(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "acf29c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def scrapeDataFromKayak(url1):\n",
    "    headers = {\n",
    "        \"accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "        \"accept-language\": \"en-US,en;q=0.9,pl;q=0.8\",\n",
    "        \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\"\n",
    "    }\n",
    "    deptime = []\n",
    "    arrtime = []\n",
    "    meridiem = []\n",
    "    price = []\n",
    "    \n",
    "    r = requests.get(url1, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    deptimes = soup.find_all('span', attrs={'class': 'airport-name'})\n",
    "\n",
    "    for div in deptimes:\n",
    "        str = div.getText().split('\\n')\n",
    "        deptime.append(str[1])\n",
    "    print(deptime)\n",
    "\n",
    "url = 'https://www.skyscanner.com/transport/flights/bos/dfw/221130'\n",
    "scrapeDataFromKayak(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd286fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
